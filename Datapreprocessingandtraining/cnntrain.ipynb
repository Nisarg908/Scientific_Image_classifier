{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T23:18:26.283782Z","iopub.status.busy":"2024-03-01T23:18:26.283048Z","iopub.status.idle":"2024-03-01T23:18:26.290296Z","shell.execute_reply":"2024-03-01T23:18:26.289360Z","shell.execute_reply.started":"2024-03-01T23:18:26.283752Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import random\n","import sklearn.model_selection as model_selection\n","import logging\n","from keras.applications.vgg16 import VGG16\n","from keras.layers import Input, Flatten, Dense\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T23:19:34.174035Z","iopub.status.busy":"2024-03-01T23:19:34.173667Z","iopub.status.idle":"2024-03-01T23:19:35.439226Z","shell.execute_reply":"2024-03-01T23:19:35.438330Z","shell.execute_reply.started":"2024-03-01T23:19:34.174010Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import os\n","from sklearn.model_selection import train_test_split\n","import cv2\n","import random\n","\n","img_width, img_height = 150, 150\n","input_shape = (img_width, img_height, 3)\n","batch_size = 128\n","epochs = 10\n","num_classes = 5\n","\n","# train_datagen = ImageDataGenerator(\n","#     rescale=1. / 255,\n","#     shear_range=0.2,\n","#     zoom_range=0.2,\n","#     horizontal_flip=True,\n","#     validation_split=0.2\n","# )  \n","\n","def prepare_file_paths(categories, base_dir):\n","    file_paths = []\n","    labels = []\n","    \n","    label_mapping = {'BlotGel': 0, 'FACS': 1, 'Macroscopy': 2, 'Microscopy': 3, 'Noneoftheabove':4}\n","\n","    for category in categories:\n","        path = os.path.join(base_dir, f\"{category}\")\n","        category_label = label_mapping[category]\n","        \n","        for img in os.listdir(path):\n","            file_paths.append(os.path.join(path, img))\n","            labels.append(category_label)\n","\n","    return file_paths, labels\n","\n","def match_class_counts(paths, labels):\n","    class_counts = {class_name: labels.count(class_name) for class_name in set(labels)}\n","    print(class_counts)\n","    for key, value in class_counts.items():\n","        print(f\"{key} -> {value}\")\n","    return paths, labels, class_counts\n","\n","def create_generator_with_balanced_augmentation(paths, labels, class_counts, batch_size):\n","    num_samples = len(paths)\n","    steps = num_samples // batch_size\n","    remaining_paths = num_samples % batch_size\n","\n","    majority_class = max(class_counts, key=class_counts.get)\n","    oversampling_images = {class_name: ((class_counts[majority_class] - class_counts[class_name]) // steps) for class_name in class_counts}\n","\n","    datagen = ImageDataGenerator(\n","        brightness_range=[0.8, 1.2],\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True\n","    )\n","\n","    while True:\n","        for i in range(steps + 1):\n","            if i != steps:\n","                batch_paths = paths[i * batch_size: (i + 1) * batch_size]\n","                batch_labels = labels[i * batch_size: (i + 1) * batch_size]\n","            else:\n","                batch_paths = paths[i * batch_size: (i * batch_size) + remaining_paths]\n","                batch_labels = labels[i * batch_size: (i * batch_size) + remaining_paths]\n","\n","            images = []\n","            for path in batch_paths:\n","                try:\n","                    rawdata = cv2.imread(path)\n","                    resized_image = cv2.resize(rawdata, (img_width, img_height))\n","                    images.append(resized_image)\n","                except Exception as e:\n","                    print(\"Error\")\n","                    break\n","            \n","            if i != steps:\n","                class_indices = {label: [] for label in oversampling_images.keys()}\n","                for idx, label in enumerate(batch_labels):\n","                    class_indices[label].append(idx)\n","\n","                for label in oversampling_images.keys():\n","                    num_to_augment = oversampling_images[label]\n","                    if num_to_augment != 0:\n","                        if len(class_indices[label]) <= num_to_augment:\n","                            num_to_augment = len(class_indices[label])\n","                        indices_to_augment = random.sample(class_indices[label], num_to_augment)\n","                        images_path_to_augment = [batch_paths[idx] for idx in indices_to_augment]\n","                        labels_of_images = [batch_labels[idx] for idx in indices_to_augment]\n","\n","                        augmented_images_per_label = []\n","                        for path in images_path_to_augment:\n","                            try:\n","                                rawdata = cv2.imread(path)\n","                                augmented_image = datagen.random_transform(rawdata)\n","                                augmented_image = np.clip(augmented_image, 0, 255)\n","                                new_data = cv2.resize(augmented_image, (img_width, img_height))\n","                                augmented_images_per_label.append(new_data)\n","                            except Exception as e:\n","                                print(f\"Error processing image: {e}\")\n","\n","                        images.extend(augmented_images_per_label)\n","                        batch_labels.extend(labels_of_images)\n","            \n","            batch_images = np.array(images).reshape(-1, img_width, img_height, 3)\n","            batch_images = batch_images / 255.0\n","\n","            batch_labels = np.array(batch_labels)\n","            \n","            yield batch_images, batch_labels\n","\n","\n","def create_generator(paths, labels, batch_size):\n","    num_samples = len(paths)\n","    steps = num_samples // batch_size\n","    remaining_paths = num_samples % batch_size\n","    while True:\n","        for i in range(steps+1):\n","            if i != steps:\n","                batch_paths = paths[i * batch_size: (i + 1) * batch_size]\n","                batch_labels = labels[i * batch_size: (i + 1) * batch_size]\n","            else:\n","                batch_paths = paths[i * batch_size: (i * batch_size)+remaining_paths]\n","                batch_labels = labels[i * batch_size: (i * batch_size)+remaining_paths]\n","\n","            images = []\n","            for path in batch_paths:\n","                try:\n","                    rawdata = cv2.imread(path)\n","                    resized_image = cv2.resize(rawdata, (img_width, img_height))\n","                    images.append(resized_image)\n","                except Exception as e:\n","                    pass\n","\n","            batch_images = np.array(images).reshape(-1, img_width, img_height, 3)\n","            batch_images = batch_images / 255.0\n","\n","            yield batch_images, np.array(batch_labels)\n","\n","# Categories\n","categories = [\"BlotGel\", \"FACS\", \"Macroscopy\", \"Microscopy\", \"Noneoftheabove\"]\n","base_dir = \"./classified_images/\"\n","\n","# Prepare file paths and labels\n","file_paths, labels = prepare_file_paths(categories, base_dir)\n","\n","# Split each view's data into train, validation, and test sets\n","train_paths, test_paths, train_labels, test_labels = train_test_split(file_paths, labels, test_size=0.15, random_state=42)\n","train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=0.15, random_state=42)\n","\n","# Match class counts for each view\n","train_paths, train_labels, class_counts = match_class_counts(train_paths, train_labels)\n","\n","# Create generators for train, validation, and test data for each view with augmentation based on the highest count\n","batch_size = 128\n","train_generator = create_generator_with_balanced_augmentation(train_paths, train_labels, class_counts, batch_size)\n","val_generator = create_generator(val_paths, val_labels, batch_size)\n","test_generator = create_generator(test_paths, test_labels, batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T23:19:46.061149Z","iopub.status.busy":"2024-03-01T23:19:46.060824Z","iopub.status.idle":"2024-03-01T23:19:46.962223Z","shell.execute_reply":"2024-03-01T23:19:46.960815Z","shell.execute_reply.started":"2024-03-01T23:19:46.061126Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n","from tensorflow.keras import regularizers\n","import numpy as np\n","import warnings\n","\n","# Function to create a CNN model with regularized layers\n","def create_cnn_model(input_shape):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(Dense(5, activation='softmax')) \n","\n","    return model\n","\n","# Function to train model with callbacks\n","def train_model_with_callbacks(model, train_paths, val_paths, train_generator, val_generator, model_name, batch_size, num_epochs):\n","\n","    best_model_path = f'{model_name}_cnn_best.h5'\n","    checkpoint_acc = ModelCheckpoint(best_model_path, monitor='val_sparse_categorical_accuracy', save_best_only=True, mode='max', verbose=1)\n","\n","    try:\n","        model.load_weights(best_model_path)\n","        print(\"Loaded the best model\")\n","    except Exception as e:\n","        print(\"No best model found. Training from scratch.\")\n","        print(e)\n","\n","    steps_per_epoch = len(train_paths) // batch_size\n","    validation_steps = len(val_paths) // batch_size\n","\n","    # Check for remaining samples and add an extra step if necessary\n","    if len(train_paths) % batch_size != 0:\n","        steps_per_epoch += 1\n","\n","    if len(val_paths) % batch_size != 0:\n","        validation_steps += 1\n","\n","    history = model.fit(train_generator, epochs=num_epochs, steps_per_epoch=steps_per_epoch, validation_data=val_generator, validation_steps=validation_steps, callbacks=[checkpoint_acc])\n","\n","    return history\n","\n","# Rest of the code remains the same...\n","batch_size = 128\n","num_epochs = 10  # Set an appropriate number of epochs\n","\n","model = create_cnn_model(input_shape)\n","\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n","\n","# Train and save the best model\n","history = train_model_with_callbacks(model, train_paths, val_paths, train_generator, val_generator, 'minded', batch_size, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the number of steps based on the test paths and batch size \n","num_steps = len(test_paths) // batch_size\n","# Check for remaining samples and add an extra step if necessary\n","if len(test_paths) % batch_size != 0:\n","    num_steps += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CODE for confusion matrix and classification details:\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","from tensorflow.keras.models import load_model\n","import matplotlib.pyplot as plt\n","import seaborn as sns  \n","# Load the models\n","# axial_best_model = load_model('/kaggle/working/axial_best_acc.h5')\n","# sagittal_best_model = load_model('/kaggle/working/sagittal_best_acc.h5')\n","# coronal_best_model = load_model('/kaggle/working/coronal_best_acc.h5')\n","\n","# Function to plot a confusion matrix\n","def plot_confusion_matrix(confusion_matrix, categories):\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=categories, yticklabels=categories)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","# Axial Model\n","#num_steps_axial = len(axial_test_paths) // batch_size\n","predictions = model.predict(test_generator, steps=num_steps)\n","predicted_labels = np.argmax(predictions, axis=1)\n","confusion = confusion_matrix(test_labels, predicted_labels)\n","classification_report = classification_report(test_labels, predicted_labels, target_names=categories)\n","\n","print(\"Classification Report:\")\n","print(classification_report)\n","\n","# Plotting Confusion Matrix\n","plot_confusion_matrix(confusion, categories)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4522110,"sourceId":7737458,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
