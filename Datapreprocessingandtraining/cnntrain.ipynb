{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T23:18:26.283782Z","iopub.status.busy":"2024-03-01T23:18:26.283048Z","iopub.status.idle":"2024-03-01T23:18:26.290296Z","shell.execute_reply":"2024-03-01T23:18:26.289360Z","shell.execute_reply.started":"2024-03-01T23:18:26.283752Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import random\n","import sklearn.model_selection as model_selection\n","import logging\n","from keras.applications.vgg16 import VGG16\n","from keras.layers import Input, Flatten, Dense\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T23:19:34.174035Z","iopub.status.busy":"2024-03-01T23:19:34.173667Z","iopub.status.idle":"2024-03-01T23:19:35.439226Z","shell.execute_reply":"2024-03-01T23:19:35.438330Z","shell.execute_reply.started":"2024-03-01T23:19:34.174010Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 19859, 1: 775, 2: 856, 3: 13033, 4: 749}\n","0 -> 19859\n","1 -> 775\n","2 -> 856\n","3 -> 13033\n","4 -> 749\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import os\n","from sklearn.model_selection import train_test_split\n","import cv2\n","import random\n","\n","img_width, img_height = 150, 150\n","input_shape = (img_width, img_height, 3)\n","batch_size = 128\n","epochs = 10\n","num_classes = 5\n","\n","# train_datagen = ImageDataGenerator(\n","#     rescale=1. / 255,\n","#     shear_range=0.2,\n","#     zoom_range=0.2,\n","#     horizontal_flip=True,\n","#     validation_split=0.2\n","# )  \n","\n","def prepare_file_paths(categories, base_dir):\n","    file_paths = []\n","    labels = []\n","    \n","    label_mapping = {'BlotGel': 0, 'FACS': 1, 'Macroscopy': 2, 'Microscopy': 3, 'Noneoftheabove':4}\n","\n","    for category in categories:\n","        path = os.path.join(base_dir, f\"{category}\")\n","        category_label = label_mapping[category]\n","        \n","        for img in os.listdir(path):\n","            file_paths.append(os.path.join(path, img))\n","            labels.append(category_label)\n","\n","    return file_paths, labels\n","\n","def match_class_counts(paths, labels):\n","    class_counts = {class_name: labels.count(class_name) for class_name in set(labels)}\n","    print(class_counts)\n","    for key, value in class_counts.items():\n","        print(f\"{key} -> {value}\")\n","    return paths, labels, class_counts\n","\n","def create_generator_with_balanced_augmentation(paths, labels, class_counts, batch_size):\n","    num_samples = len(paths)\n","    steps = num_samples // batch_size\n","    remaining_paths = num_samples % batch_size\n","\n","    majority_class = max(class_counts, key=class_counts.get)\n","    oversampling_images = {class_name: ((class_counts[majority_class] - class_counts[class_name]) // steps) for class_name in class_counts}\n","\n","    datagen = ImageDataGenerator(\n","        brightness_range=[0.8, 1.2],\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True\n","    )\n","\n","    while True:\n","        for i in range(steps + 1):\n","            if i != steps:\n","                batch_paths = paths[i * batch_size: (i + 1) * batch_size]\n","                batch_labels = labels[i * batch_size: (i + 1) * batch_size]\n","            else:\n","                batch_paths = paths[i * batch_size: (i * batch_size) + remaining_paths]\n","                batch_labels = labels[i * batch_size: (i * batch_size) + remaining_paths]\n","\n","            images = []\n","            for path in batch_paths:\n","                try:\n","                    rawdata = cv2.imread(path)\n","                    resized_image = cv2.resize(rawdata, (img_width, img_height))\n","                    images.append(resized_image)\n","                except Exception as e:\n","                    print(\"Error\")\n","                    break\n","            \n","            if i != steps:\n","                class_indices = {label: [] for label in oversampling_images.keys()}\n","                for idx, label in enumerate(batch_labels):\n","                    class_indices[label].append(idx)\n","\n","                for label in oversampling_images.keys():\n","                    num_to_augment = oversampling_images[label]\n","                    if num_to_augment != 0:\n","                        if len(class_indices[label]) <= num_to_augment:\n","                            num_to_augment = len(class_indices[label])\n","                        indices_to_augment = random.sample(class_indices[label], num_to_augment)\n","                        images_path_to_augment = [batch_paths[idx] for idx in indices_to_augment]\n","                        labels_of_images = [batch_labels[idx] for idx in indices_to_augment]\n","\n","                        augmented_images_per_label = []\n","                        for path in images_path_to_augment:\n","                            try:\n","                                rawdata = cv2.imread(path)\n","                                augmented_image = datagen.random_transform(rawdata)\n","                                augmented_image = np.clip(augmented_image, 0, 255)\n","                                new_data = cv2.resize(augmented_image, (img_width, img_height))\n","                                augmented_images_per_label.append(new_data)\n","                            except Exception as e:\n","                                print(f\"Error processing image: {e}\")\n","\n","                        images.extend(augmented_images_per_label)\n","                        batch_labels.extend(labels_of_images)\n","            \n","            batch_images = np.array(images).reshape(-1, img_width, img_height, 3)\n","            batch_images = batch_images / 255.0\n","\n","            batch_labels = np.array(batch_labels)\n","            \n","            yield batch_images, batch_labels\n","\n","\n","def create_generator(paths, labels, batch_size):\n","    num_samples = len(paths)\n","    steps = num_samples // batch_size\n","    remaining_paths = num_samples % batch_size\n","    while True:\n","        for i in range(steps+1):\n","            if i != steps:\n","                batch_paths = paths[i * batch_size: (i + 1) * batch_size]\n","                batch_labels = labels[i * batch_size: (i + 1) * batch_size]\n","            else:\n","                batch_paths = paths[i * batch_size: (i * batch_size)+remaining_paths]\n","                batch_labels = labels[i * batch_size: (i * batch_size)+remaining_paths]\n","\n","            images = []\n","            for path in batch_paths:\n","                try:\n","                    rawdata = cv2.imread(path)\n","                    resized_image = cv2.resize(rawdata, (img_width, img_height))\n","                    images.append(resized_image)\n","                except Exception as e:\n","                    pass\n","\n","            batch_images = np.array(images).reshape(-1, img_width, img_height, 3)\n","            batch_images = batch_images / 255.0\n","\n","            yield batch_images, np.array(batch_labels)\n","\n","# Categories\n","categories = [\"BlotGel\", \"FACS\", \"Macroscopy\", \"Microscopy\", \"Noneoftheabove\"]\n","base_dir = \"./classified_images/\"\n","\n","# Prepare file paths and labels\n","file_paths, labels = prepare_file_paths(categories, base_dir)\n","\n","# Split each view's data into train, validation, and test sets\n","train_paths, test_paths, train_labels, test_labels = train_test_split(file_paths, labels, test_size=0.15, random_state=42)\n","train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=0.15, random_state=42)\n","\n","# Match class counts for each view\n","train_paths, train_labels, class_counts = match_class_counts(train_paths, train_labels)\n","\n","# Create generators for train, validation, and test data for each view with augmentation based on the highest count\n","batch_size = 128\n","train_generator = create_generator_with_balanced_augmentation(train_paths, train_labels, class_counts, batch_size)\n","val_generator = create_generator(val_paths, val_labels, batch_size)\n","test_generator = create_generator(test_paths, test_labels, batch_size)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T23:19:46.061149Z","iopub.status.busy":"2024-03-01T23:19:46.060824Z","iopub.status.idle":"2024-03-01T23:19:46.962223Z","shell.execute_reply":"2024-03-01T23:19:46.960815Z","shell.execute_reply.started":"2024-03-01T23:19:46.061126Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["No best model found. Training from scratch.\n","[Errno 2] Unable to open file (unable to open file: name = 'minded_cnn_best.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n","\n","Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n","Epoch 1/10\n","276/276 [==============================] - ETA: 0s - loss: 1116.9119 - sparse_categorical_accuracy: 0.5907\n","Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.29157, saving model to minded_cnn_best.h5\n","276/276 [==============================] - 114s 410ms/step - loss: 1116.9119 - sparse_categorical_accuracy: 0.5907 - val_loss: 10458.3887 - val_sparse_categorical_accuracy: 0.2916 - lr: 0.0010\n","dict_keys(['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy', 'lr'])\n","Epoch 1, Validation Accuracy: 0.29156625270843506\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["/Users/shloksanghvi/anaconda3/envs/tensor/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["183/276 [==================>...........] - ETA: 47s - loss: 121151.8750 - sparse_categorical_accuracy: 0.5559"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Train and save the best models for each view\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mminded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[8], line 88\u001b[0m, in \u001b[0;36mtrain_model_with_callbacks\u001b[0;34m(model, train_paths, val_paths, train_generator, val_generator, model_name, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_paths) \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     86\u001b[0m     validation_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 88\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Plot learning rate vs epochs\u001b[39;00m\n\u001b[1;32m     91\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs_list, learning_rates, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n","File \u001b[0;32m~/anaconda3/envs/tensor/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n","from tensorflow.keras import regularizers\n","import numpy as np\n","import warnings\n","\n","# Function to create a CNN model with regularized layers\n","def create_cnn_model(input_shape):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Conv2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(Dense(5, activation='softmax'))  # 4 classes for AD, LMCI, EMCI, CN\n","\n","    return model\n","\n","# Function to train model with callbacks\n","def train_model_with_callbacks(model, train_paths, val_paths, train_generator, val_generator, model_name, batch_size, num_epochs):\n","    initial_learning_rate = 0.001\n","    decay_factor = 0.5\n","    patience_epochs = 5\n","    min_accuracy_increase = 0.01  # Minimum increase in validation accuracy to reset patience\n","    current_patience = 0\n","    best_val_accuracy = 0.0  # Track the best validation accuracy\n","    # List to store learning rates and epochs\n","    learning_rates = []\n","    epochs_list = []\n","\n","    def lr_decay(epoch, current_lr):\n","        nonlocal current_patience, best_val_accuracy\n","\n","        # Monitor the validation accuracy after the first epoch\n","        if epoch > 0:\n","            val_accuracy = model.history.history['val_sparse_categorical_accuracy'][-1]\n","            print(model.history.history.keys())\n","            print(f'Epoch {epoch}, Validation Accuracy: {val_accuracy}')\n","\n","            if val_accuracy - best_val_accuracy > min_accuracy_increase:\n","                # If there is a significant increase in validation accuracy, reset patience\n","                current_patience = 0\n","                best_val_accuracy = val_accuracy\n","            elif current_patience >= patience_epochs:\n","                # If patience is exhausted, decay the learning rate\n","                new_lr = current_lr * decay_factor\n","                current_patience = 0\n","                if best_val_accuracy < val_accuracy:\n","                    best_val_accuracy = val_accuracy\n","                epochs_list.append(epoch)  # Store the epoch\n","                learning_rates.append(new_lr)  # Store the learning rate\n","                print(f'Reducing learning rate to: {new_lr}')\n","                return new_lr\n","            else:\n","                # Increment patience if there is no improvement in validation accuracy\n","                current_patience += 1\n","\n","        return current_lr\n","\n","\n","    lr_scheduler = LearningRateScheduler(lambda epoch, lr: lr_decay(epoch, lr), verbose=1)\n","    best_model_path = f'{model_name}_cnn_best.h5'\n","    checkpoint_acc = ModelCheckpoint(best_model_path, monitor='val_sparse_categorical_accuracy', save_best_only=True, mode='max', verbose=1)\n","\n","    try:\n","        model.load_weights(best_model_path)\n","        print(\"Loaded the best model\")\n","    except Exception as e:\n","        print(\"No best model found. Training from scratch.\")\n","        print(e)\n","\n","    steps_per_epoch = len(train_paths) // batch_size\n","    validation_steps = len(val_paths) // batch_size\n","\n","    # Check for remaining samples and add an extra step if necessary\n","    if len(train_paths) % batch_size != 0:\n","        steps_per_epoch += 1\n","\n","    if len(val_paths) % batch_size != 0:\n","        validation_steps += 1\n","\n","    history = model.fit(train_generator, epochs=num_epochs, steps_per_epoch=steps_per_epoch, validation_data=val_generator, validation_steps=validation_steps, callbacks=[checkpoint_acc, lr_scheduler])\n","\n","    # Plot learning rate vs epochs\n","    plt.plot(epochs_list, learning_rates, marker='o', linestyle='-', color='b')\n","    plt.title('Learning Rate vs Epochs')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Learning Rate')\n","    plt.grid(True)\n","    plt.show()\n","\n","    return history\n","\n","# # Function to train model with callbacks\n","# def train_model_with_callbacks(model, train_paths, val_paths, train_generator, val_generator, model_name, batch_size, num_epochs):\n","#     initial_learning_rate = 0.001\n","#     decay_factor = 0.5\n","#     patience_epochs = 5\n","#     current_patience = 0\n","\n","#     def lr_decay(epoch, current_lr):\n","#         nonlocal current_patience\n","\n","#         if current_patience >= patience_epochs:\n","#             new_lr = current_lr * decay_factor\n","#             current_patience = 0\n","#             print(f'Reducing learning rate to: {new_lr}')\n","#             return new_lr\n","#         else:\n","#             current_patience += 1\n","#             return current_lr\n","\n","#     lr_scheduler = LearningRateScheduler(lr_decay, verbose=1)\n","#     best_model_path = '/kaggle/working/' + f'{model_name}_best.h5'\n","#     checkpoint_acc = ModelCheckpoint(best_model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n","\n","#     try:\n","#         model.load_weights(best_model_path)\n","#         print(\"Loaded the best model\")\n","#     except Exception as e:\n","#         print(\"No best model found. Training from scratch.\")\n","#         print(e)\n","\n","#     # Compile the model\n","#     #model.compile(optimizer=Adam(learning_rate=initial_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","#     steps_per_epoch = len(train_paths) // batch_size\n","#     validation_steps = len(val_paths) // batch_size\n","\n","#     # Check for remaining samples and add an extra step if necessary\n","#     if len(train_paths) % batch_size != 0:\n","#         steps_per_epoch += 1\n","\n","#     if len(val_paths) % batch_size != 0:\n","#         validation_steps += 1\n","\n","# #     steps_per_epoch = len(train_paths) // batch_size + 1\n","# #     validation_steps = len(val_paths) // batch_size + 1\n","\n","#     history = model.fit(train_generator, epochs=num_epochs, steps_per_epoch=steps_per_epoch, validation_data=val_generator, validation_steps=validation_steps, callbacks=[checkpoint_acc, lr_scheduler])\n","\n","#     return history\n","\n","# Rest of the code remains the same...\n","batch_size = 128\n","num_epochs = 10  # Set an appropriate number of epochs\n","\n","model = create_cnn_model(input_shape)\n","\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n","\n","# Train and save the best models for each view\n","history = train_model_with_callbacks(model, train_paths, val_paths, train_generator, val_generator, 'minded', batch_size, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the number of steps based on the test paths and batch size \n","num_steps = len(test_paths) // batch_size\n","# Check for remaining samples and add an extra step if necessary\n","if len(test_paths) % batch_size != 0:\n","    num_steps += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CODE for confusion matrix and classification details:\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","from tensorflow.keras.models import load_model\n","import matplotlib.pyplot as plt\n","import seaborn as sns  \n","# Load the models\n","# axial_best_model = load_model('/kaggle/working/axial_best_acc.h5')\n","# sagittal_best_model = load_model('/kaggle/working/sagittal_best_acc.h5')\n","# coronal_best_model = load_model('/kaggle/working/coronal_best_acc.h5')\n","\n","# Function to plot a confusion matrix\n","def plot_confusion_matrix(confusion_matrix, categories):\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=categories, yticklabels=categories)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","# Axial Model\n","#num_steps_axial = len(axial_test_paths) // batch_size\n","predictions = model.predict(test_generator, steps=num_steps)\n","predicted_labels = np.argmax(predictions, axis=1)\n","confusion = confusion_matrix(test_labels, predicted_labels)\n","classification_report = classification_report(test_labels, predicted_labels, target_names=categories)\n","\n","print(\"Classification Report:\")\n","print(classification_report)\n","\n","# Plotting Confusion Matrix\n","plot_confusion_matrix(confusion, categories)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4522110,"sourceId":7737458,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
